{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1ySTfz3d1kA5RQ5cDGNsXJsUNXIyz7mB6",
      "authorship_tag": "ABX9TyOEmUAb53NSzeKoE2eLE2UH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw4i5yMpZ9Ro",
        "outputId": "b231c696-7eb2-4173-fe36-36fb2672f1de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.9.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anndata>=0.7.4 (from scanpy)\n",
            "  Downloading anndata-0.9.2-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.10.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.12.2)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.56.4)\n",
            "Collecting umap-learn>=0.3.10 (from scanpy)\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scanpy) (23.1)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.41.0->scanpy) (67.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->scanpy) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.3.10->scanpy)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.9.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: umap-learn, session-info, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82807 sha256=b08514191f7b374f7031ffaa1c783d9b0be75e7eb9631954fdbe6cfd5b81faf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8026 sha256=c27d5344efa013d87e4d3fb5290c8679629f5067b66aad667f26f10954adb92e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=6f3ac98ed12a0cd82aaeb91d5c4617929b58302fe31de872d216afb4d04998cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn session-info pynndescent\n",
            "Installing collected packages: stdlib_list, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.9.2 pynndescent-0.5.10 scanpy-1.9.4 session-info-1.0.0 stdlib_list-0.9.0 umap-learn-0.5.3\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0p6NNxiEjopk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import scanpy as sc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from einops import rearrange\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 8 # The number of cells for training in one epoch.\n",
        "tr_ratio= 0.7 # train:valid\n",
        "masking_ratio = 0.15 # geneformer中loss降得最低的\n",
        "max_gs = 300 # The max of pathway/token number.即cell embedding的输出维度\n",
        "n_embd = 48 # cellembedding numbers = weight matrix numbers\n",
        "n_head = 4\n",
        "n_layer = 2 # block number\n",
        "dropout = 0.2\n",
        "epochs = 1000 # 总训练次数。因学习率降低需要增加训练次数\n",
        "eval_interval = 100 # 执行评估loss的训练次数迭代间隔\n",
        "eval_iters = 100 # 对loss求平均的验证次数\n",
        "learning_rate = 3e-4 # attention不能承受太大的学习率"
      ],
      "metadata": {
        "id": "4rItN51jzSQG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 固定随机种子\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1464)"
      ],
      "metadata": {
        "id": "WzQew_Zy5twz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# input data process"
      ],
      "metadata": {
        "id": "f7yO9CAy8o90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取原始单细胞数据\n",
        "adata = sc.read('drive/MyDrive/data/demo_train.h5ad')\n",
        "adata = adata[:,adata.var_names]\n",
        "print(adata)\n",
        "print(adata.obs.Celltype.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxL3_SdaZUZT",
        "outputId": "819c22ec-6e29-4edf-e3fa-02b02acfcb0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View of AnnData object with n_obs × n_vars = 10600 × 3000\n",
            "    obs: 'Celltype'\n",
            "    var: 'Gene Symbol'\n",
            "alpha          3136\n",
            "beta           2966\n",
            "ductal         1290\n",
            "acinar         1144\n",
            "delta           793\n",
            "PSC             524\n",
            "PP              356\n",
            "endothelial     273\n",
            "macrophage       52\n",
            "mast             25\n",
            "epsilon          21\n",
            "schwann          13\n",
            "t_cell            7\n",
            "Name: Celltype, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def toExp(adata):\n",
        "  def todense(adata):\n",
        "    \"\"\"\n",
        "    转换为表达矩阵，行是基因，列是细胞\n",
        "    \"\"\"\n",
        "    import scipy\n",
        "    if isinstance(adata.X, scipy.sparse.csr_matrix) or isinstance(adata.X, scipy.sparse.csc_matrix):\n",
        "        return adata.X.todense()\n",
        "    else:\n",
        "        return adata.X\n",
        "  el_data = pd.DataFrame(todense(adata),index=np.array(adata.obs_names).tolist(), columns=np.array(adata.var_names).tolist())\n",
        "  el_data['Celltype'] = adata.obs['Celltype'].astype('str') #最后一列添加cell type\n",
        "  genes = el_data.columns.values[:-1]\n",
        "  return el_data, genes\n",
        "\n",
        "def balance_populations(data):\n",
        "  \"\"\"\n",
        "  让每种celltype所含细胞数相等\n",
        "  \"\"\"\n",
        "  ct_names = set(data.iloc[:,-1])\n",
        "  ct_counts = pd.value_counts(data.iloc[:,-1])\n",
        "  max_val = min(ct_counts.max(),np.int32(2000000/len(ct_counts)))\n",
        "  balanced_data=pd.DataFrame(index=range(1), columns=genes)\n",
        "  for ct in ct_names:\n",
        "      tmp = data.loc[data.Celltype == ct]\n",
        "      idx = np.random.choice(range(len(tmp)), max_val)\n",
        "      tmp_X = tmp.iloc[idx,:]\n",
        "      balanced_data = pd.concat([balanced_data,tmp_X])\n",
        "  return balanced_data.drop(balanced_data.index[0])"
      ],
      "metadata": {
        "id": "K5bZaOAnemwO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建表达矩阵，行是细胞，列是基因\n",
        "el_data, genes = toExp(adata)\n",
        "el_data = balance_populations(data = el_data)\n",
        "el_data = np.array(el_data.iloc[:,:-1])\n",
        "n_genes = len(genes)\n",
        "print(el_data)\n",
        "print(el_data.shape)\n",
        "print(genes)\n",
        "print(genes.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq-tkxY87ONP",
        "outputId": "f81668a0-70ee-4ef8-a3ff-c985a6c3a23d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        1.9713649 ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " ...\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.9642614 0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]]\n",
            "(40768, 3000)\n",
            "['COL1A1' 'COL1A2' 'PPY' ... 'C9orf135' 'GRIN2D' 'HERC5']\n",
            "(3000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train and valid splits"
      ],
      "metadata": {
        "id": "1T_XVsJuhnpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and valid splits\n",
        "train_size = int(len(el_data) * tr_ratio)\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(el_data, [train_size,len(el_data)-train_size])\n",
        "train_dataset = torch.from_numpy(np.array(train_dataset)[:,:n_genes].astype(np.float32))\n",
        "valid_dataset = torch.from_numpy(np.array(valid_dataset)[:,:n_genes].astype(np.float32))\n",
        "\n",
        "print(train_dataset.shape)\n",
        "print(train_dataset)\n",
        "print(valid_dataset.shape)\n",
        "print(valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hETU-ekNx5v_",
        "outputId": "a267dc3a-831a-4f3b-8c04-bb50b39b5719"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28537, 3000])\n",
            "tensor([[1.1370, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
            "torch.Size([12231, 3000])\n",
            "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 6.7095,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 1.5748,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 6.1754,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    generate a small batch of data of inputs x and targets y\n",
        "    \"\"\"\n",
        "    data = train_dataset if split == 'train' else valid_dataset\n",
        "    ix = torch.randint(len(data), (batch_size,))\n",
        "    x = data[ix]\n",
        "    # 生成随机mask矩阵，0表示掩盖的位置，1表示要保留的位置\n",
        "    # mask = np.random.choice([0, 1], size=y.shape, p=[masking_ratio, 1 - masking_ratio])\n",
        "    # x = y * mask\n",
        "    x = x.to(device)\n",
        "    return x # (batch_size, n_genes)"
      ],
      "metadata": {
        "id": "KAwVGdDq6zV9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model structure"
      ],
      "metadata": {
        "id": "vGLTCGLuzSZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cell embedding"
      ],
      "metadata": {
        "id": "vfhwae7RXfl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 自定义权重矩阵的前向传播和反向传播计算\n",
        "class CustomizedLinearFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    autograd function, update while training\n",
        "    \"\"\"\n",
        "    @staticmethod # 通过类名直接调用\n",
        "    def forward(ctx, input, weight):\n",
        "        output = input.mm(weight.t()) # (batch_size,n_genes) @ (n_genes,max_gs)➡️(batch_size,max_gs)\n",
        "        ctx.save_for_backward(input, weight)\n",
        "        return output # gene set token (batch_size,max_gs)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight= ctx.saved_tensors\n",
        "        grad_input = grad_weight = None\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input)\n",
        "\n",
        "        return grad_input, grad_weight # 和input，weight形状相同。传播梯度，更新参数\n",
        "\n",
        "# 创建全连接线性层，即用于处理表达矩阵并可随训练更新的权重矩阵W(k,n)\n",
        "class CustomizedLinear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomizedLinear, self).__init__()\n",
        "        self.input_features = n_genes\n",
        "        self.output_features = max_gs\n",
        "        # 随机初始化对应大小的权重矩阵，让pytorch知道跟踪该张量的梯度\n",
        "        self.weight = nn.Parameter(torch.Tensor(self.output_features, self.input_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      \"\"\"\n",
        "      初始化为小范围内的随机值\n",
        "      \"\"\"\n",
        "      stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "      self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def reset_params_pos(self):\n",
        "        \"\"\" 初始化为正值\"\"\"\n",
        "        stdv = 1./math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(0,stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.float()\n",
        "        return CustomizedLinearFunction.apply(input, self.weight)\n",
        "\n",
        "# 使用梯度检查来验证自定义线性层的梯度计算是否准确\n",
        "if __name__ == 'check grad':\n",
        "    from torch.autograd import gradcheck\n",
        "    customlinear = CustomizedLinearFunction.apply\n",
        "\n",
        "    input = (torch.randn(20,20,dtype=torch.double,requires_grad=True),\n",
        "         torch.randn(30,20,dtype=torch.double,requires_grad=True),)\n",
        "    test = gradcheck(customlinear, input, eps=1e-6, atol=1e-4)\n",
        "    print(test)\n",
        "\n",
        "class FeatureEmbed(nn.Module):\n",
        "  # 基因通过可学习的权重矩阵映射到gene set token (batch_size,max_gs,n_embd)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fe = CustomizedLinear()\n",
        "    def forward(self, x):\n",
        "      embed_outputs = []\n",
        "      weights = []\n",
        "      for i in range(n_embd):\n",
        "        output = rearrange(self.fe(x), 'h (w c) -> h c w ', c=max_gs) # (batch_size,max_gs,1)\n",
        "        embed_outputs.append(output)\n",
        "        # 获得n_embd个权重矩阵的平均\n",
        "        weight = self.fe.weight\n",
        "        weights.append(weight)\n",
        "      weights = torch.stack(weights)\n",
        "      final_output = torch.cat(embed_outputs, dim=-1)\n",
        "      return final_output, weights"
      ],
      "metadata": {
        "id": "JuykzSMNXe6b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder--multi-head self-attention"
      ],
      "metadata": {
        "id": "ILRsH0bN4zVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape # gene set token(batch_size,max_gs,n_embd)\n",
        "        k = self.key(x)   # (B,T,head_size)\n",
        "        q = self.query(x) # (B,T,head_size)\n",
        "        v = self.value(x) # (B,T,head_size)\n",
        "        a = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        a = F.softmax(a, dim=-1) # (B, T, T)\n",
        "        # weights = a\n",
        "        a = self.dropout(a)\n",
        "        out = a @ v # (B,T,T) @ (B,T,hs) -> (B,T,hs)\n",
        "        return out\n",
        "        # return out,weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.head_size=head_size\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # out,weights = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        # weights = weights.view(batch_size, self.head_size, max_gs, max_gs)\n",
        "        return out\n",
        "        # return out,weights\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation\n",
        "      Attention + FeedForward \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # out,weights = self.sa(self.ln1(x))\n",
        "        # x = x + out\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "1Bb0wd0k4r_N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "lPd7O3JhG2R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_embd, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, n_embd)  # 输出层\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        output = self.fc3(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Nr65q51PG1qu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "aLWv-ut-5CHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # embed (batch_size, n_genes)➡️(batch_size,max_gs,n_embd)\n",
        "        self.feature_embed = FeatureEmbed()\n",
        "        # encoder\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        # decoder\n",
        "        self.decoder = Decoder()\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    # 初始化权重和偏置参数\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear): # 检查module是否是线性层\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # 正态分布随机初始化权重\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias) # 偏置初始化为0\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        set_target, w = self.feature_embed(idx) # (batch_size,max_gs,n_embd), (n_embd,max_gs,n_genes)\n",
        "        x = self.blocks(set_target)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.decoder(x)\n",
        "        loss = F.mse_loss(logits, set_target)\n",
        "\n",
        "        return loss, w"
      ],
      "metadata": {
        "id": "suq8dWCI5G_y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train the model"
      ],
      "metadata": {
        "id": "pLkfMSEOziER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  \"\"\"\n",
        "  取验证次数loss的平均，减少噪音\n",
        "  \"\"\"\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X= get_batch(split)\n",
        "      loss, w = model(X)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "Zo_Efuoix7mh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer()\n",
        "m = model.to(device)\n",
        "# print model parameters\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(epochs):\n",
        "    # evaluate\n",
        "    if iter % eval_interval == 0 or iter == epochs - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    loss, w = model(xb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "RNEF7O-90DeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf779610-d751-44e6-a74d-b9ef85c85c08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.007984 M parameters\n",
            "step 0: train loss 0.1030, val loss 0.1036\n",
            "step 100: train loss 0.0245, val loss 0.0251\n",
            "step 200: train loss 0.0185, val loss 0.0192\n",
            "step 300: train loss 0.0140, val loss 0.0153\n",
            "step 400: train loss 0.0123, val loss 0.0126\n",
            "step 500: train loss 0.0109, val loss 0.0115\n",
            "step 600: train loss 0.0093, val loss 0.0100\n",
            "step 700: train loss 0.0081, val loss 0.0091\n",
            "step 800: train loss 0.0076, val loss 0.0077\n",
            "step 900: train loss 0.0070, val loss 0.0070\n",
            "step 999: train loss 0.0062, val loss 0.0064\n"
          ]
        }
      ]
    }
  ]
}